{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Introduction to Reinforcement Learning\n",
    "\n",
    "### Outline\n",
    "1 About Reinforcement Learning\n",
    "2 The Reinforcement Learning Problem\n",
    "3 Inside An RL Agent\n",
    "4 Problems within Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Faces of Reinforcement Learning\n",
    "\n",
    "![Many faces of Reinforcement Learning](./images/RL.01.01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branches of Machine Learning\n",
    "\n",
    "![Branches of Machine Learning](./images/RL.01.02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Reinforcement Learning\n",
    "\n",
    "강화 학습이 다른 기계 학습 패러다임과 다른 점은 무엇입니까?\n",
    "* 감독자는 없고 보상 신호만 있음\n",
    "* 피드백은 즉각적이지 않고 지연됨\n",
    "* 시간이 정말 중요합니다 (순차적, 비 i.i.d 데이터)\n",
    "* 에이전트의 작업은 수신하는 후속 데이터에 영향을 미칩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Reinforcement Learning\n",
    "\n",
    "* 헬리콥터에서 비행 스턴트 기동\n",
    "* 주사위 놀이에서 세계 챔피언을 물리치십시오\n",
    "* 투자 포트폴리오 관리\n",
    "* 발전소 제어\n",
    "* 휴머노이드 로봇을 걷게 하기\n",
    "* 다양한 Atari 게임을 인간보다 더 잘 플레이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards (보상)\n",
    "\n",
    "* 보상 $R_t$는 스칼라 피드백 신호입니다.\n",
    "* 에이전트가 $t$ 단계에서 얼마나 잘 수행하고 있는지 나타냅니다.\n",
    "* 에이전트의 임무는 누적 보상을 극대화하는 것입니다.\n",
    "강화 학습은 **보상 가설**을 기반으로 합니다.\n",
    "\n",
    ">**정의 (보상 가설)** : 모든 목표는 기대 누적 보상의 최대화로 설명 가능하다.\n",
    "\n",
    "당신은 이 말에 동의합니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Rewards\n",
    "\n",
    "* 헬리콥터에서 비행 스턴트 기동\n",
    " - +ve 보상 원하는 궤적을 따라가면 \n",
    " - -ve 보상 충돌에 대한 \n",
    "\n",
    "* 주사위 놀이에서 세계 챔피언을 물리치십시오\n",
    " - +/−ve 보상 게임 승/패\n",
    "\n",
    "* 투자 포트폴리오 관리\n",
    " - +ve 보상 은행에 있는 각 $\n",
    " \n",
    "* 발전소 제어\n",
    " - +ve 보상 생산력에 대한 \n",
    " - -ve 보상 안전 임계값 초과에 대한\n",
    " \n",
    "* 휴머노이드 로봇을 걷게 하기\n",
    " - +ve 보상 전진 모션에 대한\n",
    " - -ve 보상 넘어지면 \n",
    "\n",
    "* 다양한 Atari 게임을 인간보다 더 잘 플레이\n",
    " - +/−ve 보상 점수 증가/감소에 대한 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Decision Making\n",
    "\n",
    "* 목표: 미래의 총 보상을 극대화하기 위한 행동 선택\n",
    "* 행동은 장기적인 결과를 초래할 수 있습니다\n",
    "* 보상이 지연될 수 있습니다.\n",
    "* 더 장기적인 보상을 얻으려면 즉각적인 보상을 희생하는 것이 더 나을 수 있습니다.\n",
    "* 예:\n",
    " - 금융 투자(성숙하는 데 몇 달이 걸릴 수 있음)\n",
    " - 헬리콥터 급유(몇 시간 내에 충돌을 방지할 수 있음)\n",
    " - 상대방의 움직임을 차단(지금부터 많은 움직임을 얻는 데 도움이 될 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and Environment\n",
    "\n",
    "![Agent and Environment](./images/RL.01.03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and Environment\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.04.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* **에이전트**의 각 단계 t 에서:\n",
    "\n",
    " - 행동 $A_t$를 실행합니다. \n",
    " \n",
    " - 관찰 $O_t$을 받습니다. \n",
    " \n",
    " - 스칼라 보상 $R_t$를 받습니다. \n",
    "<br><br>\n",
    "* **환경**:\n",
    "\n",
    " - 행동 $A_t$ 수신\n",
    "\n",
    " - 관찰 $O_{t+1}$ 방출\n",
    " \n",
    " - 스칼라 보상 $R_{t+1}$을 방출합니다.\n",
    "<br><br>\n",
    "* $t$는 환경 단계에서 증가합니다.\n",
    "</td>\n",
    "    </tr>\n",
    "</table>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History and State\n",
    "\n",
    "* **역사**는 관찰, 행동, 보상의 연속이다\n",
    "$$ H_t = O_1, R_1, A_1, \\cdots, A_{t−1}, O_t, R_t$$\n",
    "\n",
    "* 즉, 시간 t까지의 모든 관찰 가능한 변수\n",
    "* 즉, 로봇 또는 구현된 에이전트의 감각 운동 흐름\n",
    "* 다음에 일어나는 일은 기록에 따라 다릅니다.\n",
    " - 에이전트가 작업 선택\n",
    " - 환경은 관찰/보상을 선택합니다.\n",
    "* **상태**는 다음에 일어날 일을 결정하는 데 사용되는 정보입니다.\n",
    "* 공식적으로 상태는 역사의 함수입니다.\n",
    "$$S_t = f(H_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment State\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.05.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* **환경 상태** $S_t^e$는 환경의 개인 표현입니다.\n",
    "\n",
    "* 즉, 환경이 다음 관찰/보상을 선택하는 데 사용하는 모든 데이터\n",
    "\n",
    "* 환경 상태는 일반적으로 에이전트에게 표시되지 않습니다.\n",
    "\n",
    "* $S_t^e$이 보이더라도 관련 없는 정보가 포함될 수 있습니다.\n",
    "</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent State\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.06.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* **에이전트 상태** $S_t^a$는 에이전트의 내부 표현입니다.\n",
    "\n",
    "* 즉, 에이전트가 다음 작업을 선택하기 위해 사용하는 모든 정보\n",
    "\n",
    "* 즉, 강화 학습 알고리즘에서 사용하는 정보입니다.\n",
    "\n",
    "* 역사의 어떤 함수일 수 있습니다. $$S_t^a = f(H_t)$$\n",
    "</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information State\n",
    "\n",
    "* 정보 상태(Markov 상태라고도 함)에는 기록의 모든 유용한 정보가 포함됩니다.\n",
    "\n",
    "> [정의] **상태 $S_t$**는 다음과 같은 경우에만 마르코프입니다.\n",
    "$$P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t]$$\n",
    "\n",
    "* \"미래는 현재가 주어지면 과거와 무관하다\"\n",
    "$$H_{1:t} → S_t → H_{t+1:\\infty}$$\n",
    "\n",
    "* 상태가 알려지면 역사는 버려질 수 있습니다.\n",
    "\n",
    "* 즉, 상태는 미래에 대한 충분한 통계입니다.\n",
    "\n",
    "* 환경 상태 $S_t^e$ 는 Markov입니다.\n",
    "\n",
    "* 역사 $H_t$ 는 Markov입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rat Example\n",
    "\n",
    "![Rat Example](./images/RL.01.07.png)\n",
    "\n",
    "* 에이전트 상태 = 순서대로 마지막 3개 항목이면 어떻게 됩니까?\n",
    "* 에이전트 상태 = 조명, 벨 및 레버를 포함하면 어떻게 됩니까?\n",
    "* 에이전트 상태 = 완전한 시퀀스인 경우 어떻게 됩니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Observable Environments\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.08.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* **완전한 관찰 가능성**: 에이전트가 환경 상태를 **직접** 관찰합니다.\n",
    "    $$O_t = S_t^a =S_t^e $$\n",
    "\n",
    "* 에이전트 상태 = 환경 상태 = 정보 상태\n",
    "\n",
    "* 공식적으로 이것은 **Markov Decision Process(MDP)** 입니다.\n",
    "\n",
    "* (다음 강의 및 이 강의의 대부분)\n",
    "</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially Observable Environments\n",
    "\n",
    "* **부분 관찰 가능성**: 에이전트가 환경을 **간접적**으로 관찰합니다.\n",
    " - 카메라 비전이 있는 로봇은 절대 위치를 알려주지 않습니다.\n",
    " - 거래 에이전트는 현재 가격만 관찰합니다.\n",
    " - 포커 게임 에이전트는 공개 카드만 관찰합니다.\n",
    "\n",
    "\n",
    "* 이제 에이전트 상태 $\\neq$ 환경 상태\n",
    "\n",
    "* 공식적으로 이것은 **부분적으로 관찰 가능한 Markov 결정 프로세스(POMDP)**\n",
    "입니다.\n",
    "\n",
    "* 에이전트는 자신의 상태 표현 $S^a_t$ 를 구성해야 합니다. 즉,\n",
    " - 전체 기록: $S^a_t= H_t$\n",
    " - 환경 상태에 대한 믿음: $S^a_t = (P[S^e_t = S^1], \\cdots, P[S^e_t = S^n])$\n",
    " - 순환 신경망: $S^a_t = \\sigma(S^a_{t−1}W_s + O_tW_o )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Components of an RL Agent\n",
    "\n",
    "* RL 에이전트에는 다음 구성 요소 중 하나 이상이 포함될 수 있습니다.\n",
    " - 정책: 에이전트의 행동 기능\n",
    " - 가치 함수: 각 상태 및/또는 행동이 얼마나 좋은지\n",
    " - 모델: 에이전트의 환경 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
