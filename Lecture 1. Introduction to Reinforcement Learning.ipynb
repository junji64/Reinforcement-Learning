{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Introduction to Reinforcement Learning\n",
    "\n",
    "### Outline\n",
    "1. About Reinforcement Learning\n",
    "2. The Reinforcement Learning Problem\n",
    "3. Inside An RL Agent\n",
    "4. Problems within Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Faces of Reinforcement Learning\n",
    "\n",
    "![Many faces of Reinforcement Learning](./images/RL.01.01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branches of Machine Learning\n",
    "\n",
    "![Branches of Machine Learning](./images/RL.01.02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Reinforcement Learning\n",
    "\n",
    "강화 학습이 다른 기계 학습 패러다임과 다른 점은 무엇입니까?\n",
    "* 감독자는 없고 보상 신호만 있음\n",
    "* 피드백은 즉각적이지 않고 지연됨\n",
    "* 시간이 정말 중요합니다 (순차적, 비 i.i.d 데이터)\n",
    "* 에이전트의 작업은 수신하는 후속 데이터에 영향을 미칩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Reinforcement Learning\n",
    "\n",
    "* 헬리콥터에서 비행 스턴트 기동\n",
    "* 주사위 놀이에서 세계 챔피언을 물리치십시오\n",
    "* 투자 포트폴리오 관리\n",
    "* 발전소 제어\n",
    "* 휴머노이드 로봇을 걷게 하기\n",
    "* 다양한 Atari 게임을 인간보다 더 잘 플레이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards (보상)\n",
    "\n",
    "* 보상 $R_t$는 스칼라 피드백 신호입니다.\n",
    "* 에이전트가 $t$ 단계에서 얼마나 잘 수행하고 있는지 나타냅니다.\n",
    "* 에이전트의 임무는 누적 보상을 극대화하는 것입니다.\n",
    "강화 학습은 **보상 가설**을 기반으로 합니다.\n",
    "\n",
    ">**정의 (보상 가설)** : 모든 목표는 기대 누적 보상의 최대화로 설명 가능하다.\n",
    "\n",
    "당신은 이 말에 동의합니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Rewards\n",
    "\n",
    "* 헬리콥터에서 비행 스턴트 기동\n",
    " - +ve 보상 원하는 궤적을 따라가면 \n",
    " - -ve 보상 충돌에 대한 \n",
    "\n",
    "* 주사위 놀이에서 세계 챔피언을 물리치십시오\n",
    " - +/−ve 보상 게임 승/패\n",
    "\n",
    "* 투자 포트폴리오 관리\n",
    " - +ve 보상 은행에 있는 각 $\n",
    " \n",
    "* 발전소 제어\n",
    " - +ve 보상 생산력에 대한 \n",
    " - -ve 보상 안전 임계값 초과에 대한\n",
    " \n",
    "* 휴머노이드 로봇을 걷게 하기\n",
    " - +ve 보상 전진 모션에 대한\n",
    " - -ve 보상 넘어지면 \n",
    "\n",
    "* 다양한 Atari 게임을 인간보다 더 잘 플레이\n",
    " - +/−ve 보상 점수 증가/감소에 대한 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Decision Making\n",
    "\n",
    "* 목표: 미래의 총 보상을 극대화하기 위한 행동 선택\n",
    "* 행동은 장기적인 결과를 초래할 수 있습니다\n",
    "* 보상이 지연될 수 있습니다.\n",
    "* 더 장기적인 보상을 얻으려면 즉각적인 보상을 희생하는 것이 더 나을 수 있습니다.\n",
    "* 예:\n",
    " - 금융 투자(성숙하는 데 몇 달이 걸릴 수 있음)\n",
    " - 헬리콥터 급유(몇 시간 내에 충돌을 방지할 수 있음)\n",
    " - 상대방의 움직임을 차단(지금부터 많은 움직임을 얻는 데 도움이 될 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and Environment\n",
    "\n",
    "![Agent and Environment](./images/RL.01.03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and Environment\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.04.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* **에이전트**의 각 단계 t 에서:\n",
    "\n",
    " - 행동 $A_t$를 실행합니다. \n",
    " \n",
    " - 관찰 $O_t$을 받습니다. \n",
    " \n",
    " - 스칼라 보상 $R_t$를 받습니다. \n",
    "<br><br>\n",
    "* **환경**:\n",
    "\n",
    " - 행동 $A_t$ 수신\n",
    "\n",
    " - 관찰 $O_{t+1}$ 방출\n",
    " \n",
    " - 스칼라 보상 $R_{t+1}$을 방출합니다.\n",
    "<br><br>\n",
    "* $t$는 환경 단계에서 증가합니다.\n",
    "</td>\n",
    "    </tr>\n",
    "</table>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History and State\n",
    "\n",
    "* **역사**는 관찰, 행동, 보상의 연속이다\n",
    "$$ H_t = O_1, R_1, A_1, \\cdots, A_{t−1}, O_t, R_t$$\n",
    "\n",
    "* 즉, 시간 t까지의 모든 관찰 가능한 변수\n",
    "* 즉, 로봇 또는 구현된 에이전트의 감각 운동 흐름\n",
    "* 다음에 일어나는 일은 기록에 따라 다릅니다.\n",
    " - 에이전트가 작업 선택\n",
    " - 환경은 관찰/보상을 선택합니다.\n",
    "* **상태**는 다음에 일어날 일을 결정하는 데 사용되는 정보입니다.\n",
    "* 공식적으로 상태는 역사의 함수입니다.\n",
    "$$S_t = f(H_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment State\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.05.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* **환경 상태** $S_t^e$는 환경의 개인 표현입니다.\n",
    "\n",
    "* 즉, 환경이 다음 관찰/보상을 선택하는 데 사용하는 모든 데이터\n",
    "\n",
    "* 환경 상태는 일반적으로 에이전트에게 표시되지 않습니다.\n",
    "\n",
    "* $S_t^e$이 보이더라도 관련 없는 정보가 포함될 수 있습니다.\n",
    "</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent State\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.06.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* **에이전트 상태** $S_t^a$는 에이전트의 내부 표현입니다.\n",
    "\n",
    "* 즉, 에이전트가 다음 작업을 선택하기 위해 사용하는 모든 정보\n",
    "\n",
    "* 즉, 강화 학습 알고리즘에서 사용하는 정보입니다.\n",
    "\n",
    "* 역사의 어떤 함수일 수 있습니다. $$S_t^a = f(H_t)$$\n",
    "</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information State\n",
    "\n",
    "* 정보 상태(Markov 상태라고도 함)에는 기록의 모든 유용한 정보가 포함됩니다.\n",
    "\n",
    "> [정의] **상태 $S_t$**는 다음과 같은 경우에만 마르코프입니다.\n",
    "$$P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t]$$\n",
    "\n",
    "* \"미래는 현재가 주어지면 과거와 무관하다\"\n",
    "$$H_{1:t} → S_t → H_{t+1:\\infty}$$\n",
    "\n",
    "* 상태가 알려지면 역사는 버려질 수 있습니다.\n",
    "\n",
    "* 즉, 상태는 미래에 대한 충분한 통계입니다.\n",
    "\n",
    "* 환경 상태 $S_t^e$ 는 Markov입니다.\n",
    "\n",
    "* 역사 $H_t$ 는 Markov입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rat Example\n",
    "\n",
    "![Rat Example](./images/RL.01.07.png)\n",
    "\n",
    "* 에이전트 상태 = 순서대로 마지막 3개 항목이면 어떻게 됩니까?\n",
    "* 에이전트 상태 = 조명, 벨 및 레버를 포함하면 어떻게 됩니까?\n",
    "* 에이전트 상태 = 완전한 시퀀스인 경우 어떻게 됩니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Observable Environments\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.08.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* **완전한 관찰 가능성**: 에이전트가 환경 상태를 **직접** 관찰합니다.\n",
    "    $$O_t = S_t^a =S_t^e $$\n",
    "\n",
    "* 에이전트 상태 = 환경 상태 = 정보 상태\n",
    "\n",
    "* 공식적으로 이것은 **Markov Decision Process(MDP)** 입니다.\n",
    "\n",
    "* (다음 강의 및 이 강의의 대부분)\n",
    "</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially Observable Environments\n",
    "\n",
    "* **부분 관찰 가능성**: 에이전트가 환경을 **간접적**으로 관찰합니다.\n",
    " - 카메라 비전이 있는 로봇은 절대 위치를 알려주지 않습니다.\n",
    " - 거래 에이전트는 현재 가격만 관찰합니다.\n",
    " - 포커 게임 에이전트는 공개 카드만 관찰합니다.\n",
    "\n",
    "\n",
    "* 이제 에이전트 상태 $\\neq$ 환경 상태\n",
    "\n",
    "* 공식적으로 이것은 **부분적으로 관찰 가능한 Markov 결정 프로세스(POMDP)**\n",
    "입니다.\n",
    "\n",
    "* 에이전트는 자신의 상태 표현 $S^a_t$ 를 구성해야 합니다. 즉,\n",
    " - 전체 기록: $S^a_t= H_t$\n",
    " - 환경 상태에 대한 믿음: $S^a_t = (P[S^e_t = S^1], \\cdots, P[S^e_t = S^n])$\n",
    " - 순환 신경망: $S^a_t = \\sigma(S^a_{t−1}W_s + O_tW_o )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Components of an RL Agent\n",
    "\n",
    "* RL 에이전트에는 다음 구성 요소 중 하나 이상이 포함될 수 있습니다.\n",
    " - 정책(Policy): 에이전트의 행동 기능\n",
    " - 가치 함수(Value function): 각 상태 및/또는 행동이 얼마나 좋은지\n",
    " - 모델(Model): 에이전트의 환경 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "* 정책은 에이전트의 행동입니다.\n",
    "* 이는 상태에서 행동으로의 지도입니다. 즉,\n",
    "\n",
    "* 결정적(Deterministic) 정책: $a = \\pi(s)$\n",
    "\n",
    "* 확률적(Statistic) 정책: $\\pi(a|s) = P[A_t = a|S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function\n",
    "\n",
    "* 가치 함수는 미래 보상의 예측입니다\n",
    "* 상태의 좋고 나쁨을 평가하는 데 사용됩니다.\n",
    "* 따라서 작업 중에서 선택하려면 예를 들면 다음과 같습니다.\n",
    "\n",
    "$$ v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "* **모델**은 환경이 다음에 무엇을 할 것인지 예측합니다.\n",
    "* $P$는 다음 상태를 예측합니다.\n",
    "* $R$은 다음(즉시) 보상을 예측합니다.\n",
    "\n",
    "$$ P^a_{ss'} = P[S_{t+1} = s'| S_t = s, A_t = a]$$\n",
    "\n",
    "$$R^a_s = E [R_{t+1} | S_t = s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze Example\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.09.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* 보상: 시간 단계당 -1\n",
    "\n",
    "* 행동: N, E, S, W\n",
    "\n",
    "* 상태: 에이전트의 위치\n",
    "</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze Example: Policy\n",
    "\n",
    "![Maze Example: Policy](./images/RL.01.10.png)\n",
    "\n",
    "화살표는 각 상태 $s$에 대한 정책 $π(s)$를 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze Example: Value Function\n",
    "\n",
    "![Maze Example: Value Function](./images/RL.01.11.png)\n",
    "\n",
    "숫자는 각 상태 $s$의 값 $v_π(s)$를 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze Example : Model\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.12.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* 에이전트는 환경의 내부 모델을 가질 수 있습니다.\n",
    "\n",
    "* 역학: 동작이 상태를 변경하는 방법\n",
    "\n",
    "* 보상: 각 상태에서 얼마나 많은 보상\n",
    "\n",
    "* 모델이 불완전할 수 있습니다.\n",
    "\n",
    "* 그리드 레이아웃은 전이 모델 $P^a_{ss'}$을 나타냅니다.\n",
    "\n",
    "* 숫자는 각 상태 $s$에서 즉각적인 보상 $R^a_s$를 나타냅니다(모든 $a$에 대해 동일).\n",
    "</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing RL agents\n",
    "\n",
    "* 가치 기반\n",
    " - 정책 없음(암시적)\n",
    " - 가치 함수\n",
    "\n",
    "* 정책 기반\n",
    " - 정책\n",
    " - 가치 함수 없음\n",
    "\n",
    "* 배우 평론\n",
    " - 정책\n",
    " - 가치 함수\n",
    " \n",
    "* 모델 프리\n",
    " - 정책 및/또는 가치 함수\n",
    " - 모델 없음\n",
    "\n",
    "* 모델 기반\n",
    " - 정책 및/또는 가치 함수\n",
    " - 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Agent Taxonomy (분류)\n",
    "\n",
    "![RL Agent Taxonomy](./images/RL.01.13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning and Planning\n",
    "\n",
    "순차적 의사결정의 두 가지 근본적인 문제\n",
    "\n",
    "* 강화 학습:\n",
    " - 환경은 처음에 알 수 없음\n",
    " - 에이전트는 환경과 상호 작용합니다.\n",
    " - 에이전트가 정책을 개선합니다.\n",
    "\n",
    "\n",
    "* 계획:\n",
    " - 환경 모델이 알려져 있습니다.\n",
    " - 에이전트는 해당 모델로 계산을 수행합니다(외부 상호 작용 없이).\n",
    " - 에이전트가 정책을 개선합니다.\n",
    " - 일명 숙고, 추론, 성찰, 숙고, 생각, 검색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atari Example: Reinforcement Learning\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"60%\">\n",
    "        <img src=\"./images/RL.01.14.png\">\n",
    "        </td>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* 게임 룰은 불명\n",
    "\n",
    "* 대화형 게임 플레이에서 직접 배우기\n",
    "\n",
    "* 조이스틱에서 작업 선택, 픽셀 및 점수 보기\n",
    "</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atari Example: Planning\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td align=\"left\">\n",
    "            \n",
    "* 게임의 규칙은 알려져 있습니다\n",
    "\n",
    "* 에뮬레이터 쿼리 가능\n",
    "\n",
    " - 에이전트의 두뇌 속 완벽한 모델\n",
    "\n",
    "* 상태 s에서 조치를 취하는 경우:\n",
    "\n",
    " - 다음 상태는 무엇입니까?\n",
    " \n",
    " - 점수는 어떻게 될까요?\n",
    "\n",
    "* 최적의 정책을 찾기 위한 사전 계획:\n",
    "\n",
    " - 예를 들어 나무 찾기\n",
    " \n",
    "</td>\n",
    "        <td width=\"50%\">\n",
    "        <img src=\"./images/RL.01.15.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration and Exploitation (1)\n",
    "\n",
    "* 강화 학습은 시행착오 학습과 같습니다.\n",
    "* 에이전트는 좋은 정책을 찾아야 합니다.\n",
    "* 환경에 대한 경험으로부터\n",
    "* 도중에 너무 많은 보상을 잃지 않고\n",
    "* 탐색(Exploration)은 환경에 대한 추가 정보를 찾습니다.\n",
    "* 활용(Exploitation)은 알려진 정보를 활용하여 보상을 극대화합니다.\n",
    "* 일반적으로 탐색하고 활용하는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "* 레스토랑 선택\n",
    " - 착취 좋아하는 레스토랑으로 이동\n",
    " - 탐색 새로운 레스토랑을 맛보십시오\n",
    "\n",
    "\n",
    "* 온라인 배너 광고\n",
    " - 착취 가장 성공적인 광고 표시\n",
    " - 탐색 다른 광고 표시\n",
    "\n",
    "* 석유 시추\n",
    " - 가장 잘 알려진 위치에서 착취 훈련\n",
    " - 새로운 위치에서 탐사 훈련\n",
    "\n",
    "* 게임 플레이\n",
    " - 착취 당신이 가장 좋다고 생각하는 움직임을 플레이하십시오.\n",
    " - 탐색 실험적인 움직임을 재생합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Control\n",
    "\n",
    "* 예측: 미래를 평가하다\n",
    " - 주어진 정책\n",
    "\n",
    "* 제어: 미래를 최적화\n",
    " - 최고의 정책 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld Example: Prediction\n",
    "\n",
    "![Gridworld Example: Prediction](./images/RL.01.16.png)\n",
    "\n",
    "균일 무작위 정책의 가치 함수는 무엇입니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld Example: Control\n",
    "\n",
    "![Gridworld Example: Control](./images/RL.01.17.png)\n",
    "가능한 모든 정책에 대한 최적 가치 함수는 무엇입니까?\n",
    "최적의 정책은 무엇입니까?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
